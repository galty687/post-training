1:"$Sreact.fragment"
2:I[7921,["716","static/chunks/716-8b6c239d56add41d.js","98","static/chunks/98-71e6ffba5da79e51.js","177","static/chunks/app/layout-438f3f1dcf44f88c.js"],""]
3:I[8295,["716","static/chunks/716-8b6c239d56add41d.js","98","static/chunks/98-71e6ffba5da79e51.js","177","static/chunks/app/layout-438f3f1dcf44f88c.js"],"RootProvider"]
4:I[1409,[],""]
5:I[1813,[],""]
6:I[6053,["285","static/chunks/285-5cab2c46bf0cbdc3.js","698","static/chunks/698-2edc64166f51fb90.js","997","static/chunks/997-8062435351294ce8.js","795","static/chunks/795-2e60776a7ce82cea.js","499","static/chunks/app/docs/layout-8640be9db805fb1e.js"],"TreeContextProvider"]
b:I[323,[],""]
:HL["/_next/static/media/bb3ef058b751a6ad-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/media/edef1a271f97a8ec-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/css/d0f7ca2a7022be00.css","style"]
:HL["/_next/static/css/62a6e77bcc5afb05.css","style"]
:HL["/_next/static/css/4cd23781d18a52d3.css","style"]
0:{"P":null,"b":"3EBlWFcioSMB-HhmRAEmX","p":"","c":["","docs","resources","references"],"i":false,"f":[[["",{"children":["docs",{"children":[["slug","resources/references","oc"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/d0f7ca2a7022be00.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","link","1",{"rel":"stylesheet","href":"/_next/static/css/62a6e77bcc5afb05.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","link","2",{"rel":"stylesheet","href":"/_next/static/css/4cd23781d18a52d3.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"zh-CN","className":"__variable_51740a __variable_3c557b","suppressHydrationWarning":true,"children":[["$","head",null,{"children":[["$","$L2",null,{"src":"https://www.googletagmanager.com/gtag/js?id=G-6PZFJCKFSJ","strategy":"afterInteractive"}],["$","$L2",null,{"id":"google-analytics","strategy":"afterInteractive","children":"\n            window.dataLayer = window.dataLayer || [];\n            function gtag(){dataLayer.push(arguments);}\n            gtag('js', new Date());\n            gtag('config', 'G-6PZFJCKFSJ');\n          "}]]}],["$","body",null,{"className":"flex min-h-screen flex-col","children":["$","$L3",null,{"search":{"options":{"type":"static"}},"children":["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]}]]}]]}],{"children":["docs",["$","$1","c",{"children":[null,["$","$L6",null,{"tree":{"$id":"root","name":"课程内容","children":[{"$id":"#0","type":"separator","icon":"$undefined","name":"课程内容"},{"$id":"index.mdx","type":"page","name":"课程总览","description":"大语言模型后训练实践——北京大学软件与微电子学院研究生课程","icon":"$undefined","url":"/docs","$ref":{"file":"index.mdx"}},{"$id":"#2","type":"separator","icon":"$undefined","name":"讲座"},{"type":"folder","name":"第1课：后训练概述与SFT基础","icon":"$undefined","root":"$undefined","defaultOpen":"$undefined","description":"$undefined","index":"$undefined","children":[{"$id":"lecture-1/index.mdx","type":"page","name":"第1课：后训练概述与监督微调基础","description":"理解后训练在 LLM 开发流程中的位置和核心方法体系，掌握 SFT 训练循环，配置 LoRA/QLoRA 进行参数高效训练","icon":"$undefined","url":"/docs/lecture-1","$ref":{"file":"lecture-1/index.mdx"}},{"$id":"lecture-1/post-training-overview.mdx","type":"page","name":"1.1 后训练的定义与基本流程","description":"理解 LLM 开发的三阶段流程，掌握后训练的核心方法体系，了解 Tülu 3 和 Qwen3 的后训练实践","icon":"$undefined","url":"/docs/lecture-1/post-training-overview","$ref":{"file":"lecture-1/post-training-overview.mdx"}},{"$id":"lecture-1/sft-core-concepts.mdx","type":"page","name":"1.2 监督微调核心概念","description":"掌握 ChatML 和 Llama 聊天模板格式，理解掩码损失（Masked Loss）的原理，了解数据质量的重要性","icon":"$undefined","url":"/docs/lecture-1/sft-core-concepts","$ref":{"file":"lecture-1/sft-core-concepts.mdx"}},{"$id":"lecture-1/parameter-efficient-ft.mdx","type":"page","name":"1.3 参数高效微调","description":"掌握 LoRA 和 QLoRA 的原理与实践，理解参数高效微调的必要性和显存优化策略","icon":"$undefined","url":"/docs/lecture-1/parameter-efficient-ft","$ref":{"file":"lecture-1/parameter-efficient-ft.mdx"}},{"$id":"lecture-1/evaluation-methods.mdx","type":"page","name":"1.4 模型评估方法","description":"了解 LLM 后训练的主流评估方法：LLM-as-Judge、人类偏好排行榜、能力专项基准和安全评估","icon":"$undefined","url":"/docs/lecture-1/evaluation-methods","$ref":{"file":"lecture-1/evaluation-methods.mdx"}},{"$id":"lecture-1/papers.mdx","type":"page","name":"第1课 推荐论文","description":"第1课推荐阅读的 5 篇核心论文：Tülu 3、LoRA、QLoRA、LIMA、MAGPIE","icon":"$undefined","url":"/docs/lecture-1/papers","$ref":{"file":"lecture-1/papers.mdx"}},{"$id":"lecture-1/lab.mdx","type":"page","name":"实验1：微调 Qwen3-1.7B 为指令跟随助手","description":"使用 QLoRA 和 SFTTrainer 将 Qwen3-1.7B 基座模型微调为指令跟随助手的完整实验","icon":"$undefined","url":"/docs/lecture-1/lab","$ref":{"file":"lecture-1/lab.mdx"}}],"$id":"lecture-1","$ref":{"metaFile":"lecture-1/meta.json"}},{"type":"folder","name":"第2课：SFT进阶","icon":"$undefined","root":"$undefined","defaultOpen":"$undefined","description":"$undefined","index":"$undefined","children":[{"$id":"lecture-2/index.mdx","type":"page","name":"第2课：SFT 进阶——数据工程与指令微调深入","description":"掌握指令数据集的构建与质量控制方法，理解数据混合策略，实践 LLM-as-Judge 系统化评估","icon":"$undefined","url":"/docs/lecture-2","$ref":{"file":"lecture-2/index.mdx"}},{"$id":"lecture-2/instruction-datasets.mdx","type":"page","name":"2.1 指令数据集构建方法","description":"从 Self-Instruct 到 MAGPIE 的指令数据集技术演进，数据质量控制方法，数据混合策略","icon":"$undefined","url":"/docs/lecture-2/instruction-datasets","$ref":{"file":"lecture-2/instruction-datasets.mdx"}},{"$id":"lecture-2/sft-hyperparameters.mdx","type":"page","name":"2.2 SFT 超参数实践指南","description":"基于 Pareja 等（2024）的系统实验结果，掌握 SFT 核心超参数的选择方法和常见问题诊断","icon":"$undefined","url":"/docs/lecture-2/sft-hyperparameters","$ref":{"file":"lecture-2/sft-hyperparameters.mdx"}},{"$id":"lecture-2/llm-as-judge.mdx","type":"page","name":"2.3 LLM-as-Judge 评估方法","description":"深入理解 LLM-as-Judge 评估框架，掌握 MT-Bench 评判模板，了解位置偏差和评委模型选择","icon":"$undefined","url":"/docs/lecture-2/llm-as-judge","$ref":{"file":"lecture-2/llm-as-judge.mdx"}},{"$id":"lecture-2/papers.mdx","type":"page","name":"第2课 推荐论文","description":"第2课推荐阅读的 5 篇核心论文：SFT 超参数指南、Self-Instruct、MT-Bench、UltraChat、Deita","icon":"$undefined","url":"/docs/lecture-2/papers","$ref":{"file":"lecture-2/papers.mdx"}},{"$id":"lecture-2/lab.mdx","type":"page","name":"实验2：构建领域定制 SFT 模型并系统评估","description":"从数据分析到模型训练到 LLM-as-Judge 评估的完整指令微调实验，含消融实验","icon":"$undefined","url":"/docs/lecture-2/lab","$ref":{"file":"lecture-2/lab.mdx"}}],"$id":"lecture-2","$ref":{"metaFile":"lecture-2/meta.json"}},{"type":"folder","name":"第3课：偏好对齐DPO","icon":"$undefined","root":"$undefined","defaultOpen":"$undefined","description":"$undefined","index":"$undefined","children":[{"$id":"lecture-3/index.mdx","type":"page","name":"第3课：偏好对齐——DPO 及其变体","description":"理解为什么仅靠 SFT 不足以实现对齐，从 RLHF 目标推导 DPO 损失函数，实现 DPO 训练，并对 DPO 与 SimPO 进行实证比较","icon":"$undefined","url":"/docs/lecture-3","$ref":{"file":"lecture-3/index.mdx"}},{"$id":"lecture-3/alignment-problem.mdx","type":"page","name":"3.1 对齐问题：为什么仅靠 SFT 不够","description":"SFT 教会模型'说什么'，但未教会它'如何选择'。人类偏好本质上是比较性的，偏好优化直接捕获这一信号。","icon":"$undefined","url":"/docs/lecture-3/alignment-problem","$ref":{"file":"lecture-3/alignment-problem.mdx"}},{"$id":"lecture-3/dpo-derivation.mdx","type":"page","name":"3.2 DPO 数学推导","description":"从 RLHF 目标到 DPO 损失函数的完整四步推导，包括闭式最优策略、奖励重参数化和 Bradley-Terry 代入","icon":"$undefined","url":"/docs/lecture-3/dpo-derivation","$ref":{"file":"lecture-3/dpo-derivation.mdx"}},{"$id":"lecture-3/dpo-variants.mdx","type":"page","name":"3.3 DPO 变体","description":"SimPO、KTO、ORPO、IPO 等 DPO 变体的设计理念、数学公式与适用场景比较","icon":"$undefined","url":"/docs/lecture-3/dpo-variants","$ref":{"file":"lecture-3/dpo-variants.mdx"}},{"$id":"lecture-3/practical-considerations.mdx","type":"page","name":"3.4 实践考量","description":"在线 vs 离线 DPO、beta 参数敏感性分析、偏好数据质量、当前领域趋势","icon":"$undefined","url":"/docs/lecture-3/practical-considerations","$ref":{"file":"lecture-3/practical-considerations.mdx"}},{"$id":"lecture-3/papers.mdx","type":"page","name":"第3课推荐论文","description":"DPO、SimPO、KTO、DPO vs PPO 对比实验、DPO 综述等 5 篇核心论文","icon":"$undefined","url":"/docs/lecture-3/papers","$ref":{"file":"lecture-3/papers.mdx"}},{"$id":"lecture-3/lab.mdx","type":"page","name":"第3课实验：DPO 对齐与 SimPO 对比","description":"使用 DPO 对齐 SFT 模型，与 SimPO 进行实证对比，涵盖偏好数据探索、训练、评估的完整流程","icon":"$undefined","url":"/docs/lecture-3/lab","$ref":{"file":"lecture-3/lab.mdx"}}],"$id":"lecture-3","$ref":{"metaFile":"lecture-3/meta.json"}},{"type":"folder","name":"第4课：RLHF与GRPO","icon":"$undefined","root":"$undefined","defaultOpen":"$undefined","description":"$undefined","index":"$undefined","children":[{"$id":"lecture-4/index.mdx","type":"page","name":"第4课：RLHF 原理与推理强化学习（GRPO）","description":"理解完整的 RLHF 流程，掌握 GRPO 算法，使用可验证奖励复现迷你 DeepSeek-R1-Zero 实验，观察推理能力的涌现","icon":"$undefined","url":"/docs/lecture-4","$ref":{"file":"lecture-4/index.mdx"}},{"$id":"lecture-4/rlhf-pipeline.mdx","type":"page","name":"4.1 经典 RLHF 流程","description":"InstructGPT 三阶段流程、奖励模型训练、PPO 核心组件、四模型架构及常见不稳定性","icon":"$undefined","url":"/docs/lecture-4/rlhf-pipeline","$ref":{"file":"lecture-4/rlhf-pipeline.mdx"}},{"$id":"lecture-4/grpo-reasoning.mdx","type":"page","name":"4.2 GRPO 与推理涌现","description":"DeepSeek-R1-Zero 里程碑、GRPO 四步算法详解、RLVR 可验证奖励、DeepSeek-R1 完整流程","icon":"$undefined","url":"/docs/lecture-4/grpo-reasoning","$ref":{"file":"lecture-4/grpo-reasoning.mdx"}},{"$id":"lecture-4/grpo-improvements.mdx","type":"page","name":"4.3 GRPO 改进与测试时计算","description":"DAPO、Dr. GRPO、REINFORCE++ 等改进方法，以及测试时计算扩展的概念和与 o1/o3/R1 的连接","icon":"$undefined","url":"/docs/lecture-4/grpo-improvements","$ref":{"file":"lecture-4/grpo-improvements.mdx"}},{"$id":"lecture-4/rlhf-engineering.mdx","type":"page","name":"4.4 RLHF 工程工具","description":"TRL、OpenRLHF、veRL 等 RLHF/GRPO 训练工具的功能、架构和使用场景对比","icon":"$undefined","url":"/docs/lecture-4/rlhf-engineering","$ref":{"file":"lecture-4/rlhf-engineering.mdx"}},{"$id":"lecture-4/papers.mdx","type":"page","name":"第4课推荐论文","description":"InstructGPT、DeepSeek-R1、DeepSeekMath、Snell et al.、DAPO、Safe RLHF、Qwen3 等 7 篇核心论文","icon":"$undefined","url":"/docs/lecture-4/papers","$ref":{"file":"lecture-4/papers.mdx"}},{"$id":"lecture-4/lab.mdx","type":"page","name":"第4课实验：迷你 DeepSeek-R1-Zero","description":"使用 GRPO 训练 Qwen3-1.7B-Base 进行数学推理，观察推理能力的涌现过程，并与蒸馏模型和 Qwen3 思考模式对比","icon":"$undefined","url":"/docs/lecture-4/lab","$ref":{"file":"lecture-4/lab.mdx"}}],"$id":"lecture-4","$ref":{"metaFile":"lecture-4/meta.json"}},{"type":"folder","name":"第5课：压缩部署与扩展","icon":"$undefined","root":"$undefined","defaultOpen":"$undefined","description":"$undefined","index":"$undefined","children":[{"$id":"lecture-5/index.mdx","type":"page","name":"第5课：模型压缩、部署优化与能力扩展","description":"掌握模型量化的原理与实践（INT8/INT4/GPTQ/AWQ），理解知识蒸馏在后训练中的角色，了解多模态和工具使用等能力扩展方法","icon":"$undefined","url":"/docs/lecture-5","$ref":{"file":"lecture-5/index.mdx"}},{"$id":"lecture-5/quantization.mdx","type":"page","name":"5.1 模型量化","description":"深入理解模型量化的原理、主流 PTQ 方法（INT8/INT4/GPTQ/AWQ）与 QAT，掌握精度-速度-显存的权衡","icon":"$undefined","url":"/docs/lecture-5/quantization","$ref":{"file":"lecture-5/quantization.mdx"}},{"$id":"lecture-5/knowledge-distillation.mdx","type":"page","name":"5.2 知识蒸馏","description":"理解经典知识蒸馏范式、LLM 时代的蒸馏实践（DeepSeek-R1-Distill、Qwen3 思考模式融合），以及蒸馏与 RL 训练的权衡","icon":"$undefined","url":"/docs/lecture-5/knowledge-distillation","$ref":{"file":"lecture-5/knowledge-distillation.mdx"}},{"$id":"lecture-5/capability-extensions.mdx","type":"page","name":"5.3 能力扩展概览","description":"多模态后训练（VLM/LLaVA）、工具使用与智能体（函数调用/MCP）、知识编辑（参数编辑/RAG）三大能力扩展方向","icon":"$undefined","url":"/docs/lecture-5/capability-extensions","$ref":{"file":"lecture-5/capability-extensions.mdx"}},{"$id":"lecture-5/papers.mdx","type":"page","name":"第5课 推荐论文","description":"GPTQ、AWQ、LLaVA、ToolACE、DeepSeek-R1 等 5 篇核心论文，涵盖量化、多模态、工具使用与蒸馏","icon":"$undefined","url":"/docs/lecture-5/papers","$ref":{"file":"lecture-5/papers.mdx"}},{"$id":"lecture-5/lab.mdx","type":"page","name":"第5课 上机实验","description":"量化实验（必做）：多精度加载 Qwen3-8B 并评估压缩影响；能力扩展选做（三选一）：蒸馏分析、多模态实验、工具调用实验","icon":"$undefined","url":"/docs/lecture-5/lab","$ref":{"file":"lecture-5/lab.mdx"}}],"$id":"lecture-5","$ref":{"metaFile":"lecture-5/meta.json"}},{"type":"folder","name":"第6课：项目报告与总结","icon":"$undefined","root":"$undefined","defaultOpen":"$undefined","description":"$undefined","index":"$undefined","children":[{"$id":"lecture-6/index.mdx","type":"page","name":"第6课：课程项目报告与总结","description":"课程项目报告展示、评分标准，以及后训练技术全景回顾与前沿展望","icon":"$undefined","url":"/docs/lecture-6","$ref":{"file":"lecture-6/index.mdx"}},{"$id":"lecture-6/project-directions.mdx","type":"page","name":"推荐项目方向","description":"7 个推荐的课程项目方向，涵盖数学推理、安全对齐、中文写作、代码推理、领域问答、视觉问答、工具调用智能体","icon":"$undefined","url":"/docs/lecture-6/project-directions","$ref":{"file":"lecture-6/project-directions.mdx"}},{"$id":"lecture-6/presentation-guide.mdx","type":"page","name":"演示指南与评分标准","description":"项目演示的时间安排、内容要求、评分细则，以及高质量演示的实用技巧","icon":"$undefined","url":"/docs/lecture-6/presentation-guide","$ref":{"file":"lecture-6/presentation-guide.mdx"}},{"$id":"lecture-6/course-summary.mdx","type":"page","name":"课程总结与展望","description":"后训练技术完整图谱回顾，从 SFT 到 GRPO 的完整路径，以及后训练领域的前沿研究方向","icon":"$undefined","url":"/docs/lecture-6/course-summary","$ref":{"file":"lecture-6/course-summary.mdx"}}],"$id":"lecture-6","$ref":{"metaFile":"lecture-6/meta.json"}},{"$id":"#9","type":"separator","icon":"$undefined","name":"资源"},{"type":"folder","name":"课程资源","icon":"$undefined","root":"$undefined","defaultOpen":"$undefined","description":"$undefined","index":"$undefined","children":[{"$id":"resources/setup-guide.mdx","type":"page","name":"环境配置指南","description":"AutoDL 环境搭建、Colab Pro 替代方案、依赖安装、模型预下载、一键启动脚本与验证步骤","icon":"$undefined","url":"/docs/resources/setup-guide","$ref":{"file":"resources/setup-guide.mdx"}},{"$id":"resources/gpu-cost-guide.mdx","type":"page","name":"GPU 配置与费用估算","description":"各课次 GPU 配置推荐、AutoDL 费用明细、省钱技巧与替代平台","icon":"$undefined","url":"/docs/resources/gpu-cost-guide","$ref":{"file":"resources/gpu-cost-guide.mdx"}},{"$id":"resources/references.mdx","type":"page","name":"完整参考文献","description":"按课次整理的全部推荐论文、教材和在线资源","icon":"$undefined","url":"/docs/resources/references","$ref":{"file":"resources/references.mdx"}},{"$id":"resources/grading.mdx","type":"page","name":"评分标准","description":"课程考核方式、各项评分细则与评分标准详细说明","icon":"$undefined","url":"/docs/resources/grading","$ref":{"file":"resources/grading.mdx"}}],"$id":"resources","$ref":{"metaFile":"resources/meta.json"}}]},"children":"$L7"}]]}],{"children":[["slug","resources/references","oc"],"$L8",{"children":["__PAGE__","$L9",{},null,false]},null,false]},null,false]},null,false],"$La",false]],"m":"$undefined","G":["$b",[]],"s":false,"S":true}
c:I[4564,["285","static/chunks/285-5cab2c46bf0cbdc3.js","698","static/chunks/698-2edc64166f51fb90.js","997","static/chunks/997-8062435351294ce8.js","795","static/chunks/795-2e60776a7ce82cea.js","499","static/chunks/app/docs/layout-8640be9db805fb1e.js"],"NavProvider"]
d:I[4196,["285","static/chunks/285-5cab2c46bf0cbdc3.js","698","static/chunks/698-2edc64166f51fb90.js","997","static/chunks/997-8062435351294ce8.js","795","static/chunks/795-2e60776a7ce82cea.js","499","static/chunks/app/docs/layout-8640be9db805fb1e.js"],"Navbar"]
e:I[7326,["285","static/chunks/285-5cab2c46bf0cbdc3.js","697","static/chunks/697-8c55336afc55c2a1.js","324","static/chunks/324-9e7d7f5bb89b8982.js","870","static/chunks/app/docs/%5B%5B...slug%5D%5D/page-6a2f4c9a93e41015.js"],"default"]
f:I[140,["285","static/chunks/285-5cab2c46bf0cbdc3.js","698","static/chunks/698-2edc64166f51fb90.js","997","static/chunks/997-8062435351294ce8.js","795","static/chunks/795-2e60776a7ce82cea.js","499","static/chunks/app/docs/layout-8640be9db805fb1e.js"],"SearchToggle"]
10:I[9896,["285","static/chunks/285-5cab2c46bf0cbdc3.js","698","static/chunks/698-2edc64166f51fb90.js","997","static/chunks/997-8062435351294ce8.js","795","static/chunks/795-2e60776a7ce82cea.js","499","static/chunks/app/docs/layout-8640be9db805fb1e.js"],"SidebarTrigger"]
11:I[4196,["285","static/chunks/285-5cab2c46bf0cbdc3.js","698","static/chunks/698-2edc64166f51fb90.js","997","static/chunks/997-8062435351294ce8.js","795","static/chunks/795-2e60776a7ce82cea.js","499","static/chunks/app/docs/layout-8640be9db805fb1e.js"],"LayoutBody"]
12:I[9896,["285","static/chunks/285-5cab2c46bf0cbdc3.js","698","static/chunks/698-2edc64166f51fb90.js","997","static/chunks/997-8062435351294ce8.js","795","static/chunks/795-2e60776a7ce82cea.js","499","static/chunks/app/docs/layout-8640be9db805fb1e.js"],"Sidebar"]
13:I[9896,["285","static/chunks/285-5cab2c46bf0cbdc3.js","698","static/chunks/698-2edc64166f51fb90.js","997","static/chunks/997-8062435351294ce8.js","795","static/chunks/795-2e60776a7ce82cea.js","499","static/chunks/app/docs/layout-8640be9db805fb1e.js"],"SidebarContentMobile"]
14:I[9896,["285","static/chunks/285-5cab2c46bf0cbdc3.js","698","static/chunks/698-2edc64166f51fb90.js","997","static/chunks/997-8062435351294ce8.js","795","static/chunks/795-2e60776a7ce82cea.js","499","static/chunks/app/docs/layout-8640be9db805fb1e.js"],"SidebarHeader"]
15:I[5461,["285","static/chunks/285-5cab2c46bf0cbdc3.js","698","static/chunks/698-2edc64166f51fb90.js","997","static/chunks/997-8062435351294ce8.js","795","static/chunks/795-2e60776a7ce82cea.js","499","static/chunks/app/docs/layout-8640be9db805fb1e.js"],"ThemeToggle"]
16:I[9896,["285","static/chunks/285-5cab2c46bf0cbdc3.js","698","static/chunks/698-2edc64166f51fb90.js","997","static/chunks/997-8062435351294ce8.js","795","static/chunks/795-2e60776a7ce82cea.js","499","static/chunks/app/docs/layout-8640be9db805fb1e.js"],"SidebarViewport"]
17:I[9896,["285","static/chunks/285-5cab2c46bf0cbdc3.js","698","static/chunks/698-2edc64166f51fb90.js","997","static/chunks/997-8062435351294ce8.js","795","static/chunks/795-2e60776a7ce82cea.js","499","static/chunks/app/docs/layout-8640be9db805fb1e.js"],"SidebarItem"]
18:I[9896,["285","static/chunks/285-5cab2c46bf0cbdc3.js","698","static/chunks/698-2edc64166f51fb90.js","997","static/chunks/997-8062435351294ce8.js","795","static/chunks/795-2e60776a7ce82cea.js","499","static/chunks/app/docs/layout-8640be9db805fb1e.js"],"SidebarPageTree"]
19:I[9896,["285","static/chunks/285-5cab2c46bf0cbdc3.js","698","static/chunks/698-2edc64166f51fb90.js","997","static/chunks/997-8062435351294ce8.js","795","static/chunks/795-2e60776a7ce82cea.js","499","static/chunks/app/docs/layout-8640be9db805fb1e.js"],"SidebarFooter"]
1a:I[4196,["285","static/chunks/285-5cab2c46bf0cbdc3.js","698","static/chunks/698-2edc64166f51fb90.js","997","static/chunks/997-8062435351294ce8.js","795","static/chunks/795-2e60776a7ce82cea.js","499","static/chunks/app/docs/layout-8640be9db805fb1e.js"],"CollapsibleControl"]
1b:I[9896,["285","static/chunks/285-5cab2c46bf0cbdc3.js","698","static/chunks/698-2edc64166f51fb90.js","997","static/chunks/997-8062435351294ce8.js","795","static/chunks/795-2e60776a7ce82cea.js","499","static/chunks/app/docs/layout-8640be9db805fb1e.js"],"SidebarContent"]
1c:I[9896,["285","static/chunks/285-5cab2c46bf0cbdc3.js","698","static/chunks/698-2edc64166f51fb90.js","997","static/chunks/997-8062435351294ce8.js","795","static/chunks/795-2e60776a7ce82cea.js","499","static/chunks/app/docs/layout-8640be9db805fb1e.js"],"SidebarCollapseTrigger"]
1d:I[140,["285","static/chunks/285-5cab2c46bf0cbdc3.js","698","static/chunks/698-2edc64166f51fb90.js","997","static/chunks/997-8062435351294ce8.js","795","static/chunks/795-2e60776a7ce82cea.js","499","static/chunks/app/docs/layout-8640be9db805fb1e.js"],"LargeSearchToggle"]
1f:I[3652,[],"OutletBoundary"]
21:I[1293,[],"AsyncMetadataOutlet"]
23:I[3652,[],"ViewportBoundary"]
25:I[3652,[],"MetadataBoundary"]
26:"$Sreact.suspense"
7:["$","$Lc",null,{"transparentMode":"$undefined","children":[["$","$Ld",null,{"className":"h-(--fd-nav-height) on-root:[--fd-nav-height:56px] md:on-root:[--fd-nav-height:0px] md:hidden","children":[["$","$Le",null,{"href":"/","className":"inline-flex items-center gap-2.5 font-semibold","children":"LLM 后训练实践"}],["$","div",null,{"className":"flex-1","children":"$undefined"}],["$","$Lf",null,{"className":"p-2","hideIfDisabled":true}],["$","$L10",null,{"className":"inline-flex items-center justify-center rounded-md text-sm font-medium transition-colors duration-100 disabled:pointer-events-none disabled:opacity-50 focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-fd-ring hover:bg-fd-accent hover:text-fd-accent-foreground [&_svg]:size-4.5 p-2","children":["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide","children":[[["$","rect","afitv7",{"width":"18","height":"18","x":"3","y":"3","rx":"2"}],["$","path","fh3hqa",{"d":"M9 3v18"}]],"$undefined"]}]}]]}],["$","$L11",null,{"className":"md:[&_#nd-page_article]:pt-12 xl:[--fd-toc-width:286px] xl:[&_#nd-page_article]:px-8 md:[--fd-sidebar-width:268px] lg:[--fd-sidebar-width:286px]","children":[["$","$L12",null,{"defaultOpenLevel":"$undefined","prefetch":"$undefined","Mobile":["$","$L13",null,{"children":[["$","$L14",null,{"children":[["$","div",null,{"className":"flex text-fd-muted-foreground items-center gap-1.5","children":[["$","div",null,{"className":"flex flex-1","children":[]}],null,["$","$L15",null,{"className":"p-0","mode":"$undefined"}],["$","$L10",null,{"className":"inline-flex items-center justify-center rounded-md text-sm font-medium transition-colors duration-100 disabled:pointer-events-none disabled:opacity-50 focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-fd-ring hover:bg-fd-accent hover:text-fd-accent-foreground [&_svg]:size-4.5 p-2","children":["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide","children":[[["$","rect","afitv7",{"width":"18","height":"18","x":"3","y":"3","rx":"2"}],["$","path","fh3hqa",{"d":"M9 3v18"}]],"$undefined"]}]}]]}],false,"$undefined"]}],["$","$L16",null,{"children":[[["$","$L17","0",{"href":"/docs","icon":"$undefined","external":"$undefined","className":"mb-4","children":"课程内容"}]],["$","$L18",null,{"components":"$undefined"}]]}],["$","$L19",null,{"className":"empty:hidden","children":"$undefined"}]]}],"Content":[["$","$L1a",null,{}],["$","$L1b",null,{"children":[["$","$L14",null,{"children":[["$","div",null,{"className":"flex","children":[["$","$Le",null,{"href":"/","className":"inline-flex text-[15px] items-center gap-2.5 font-medium me-auto","children":"LLM 后训练实践"}],"$undefined",["$","$L1c",null,{"className":"inline-flex items-center justify-center rounded-md text-sm font-medium transition-colors duration-100 disabled:pointer-events-none disabled:opacity-50 focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-fd-ring hover:bg-fd-accent hover:text-fd-accent-foreground p-1.5 [&_svg]:size-4.5 mb-auto text-fd-muted-foreground","children":["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide","children":[[["$","rect","afitv7",{"width":"18","height":"18","x":"3","y":"3","rx":"2"}],["$","path","fh3hqa",{"d":"M9 3v18"}]],"$undefined"]}]}]]}],["$","$L1d",null,{"hideIfDisabled":true}],false,"$undefined"]}],"$7:props:children:1:props:children:0:props:Mobile:props:children:1",["$","$L19",null,{"children":[["$","div",null,{"className":"flex text-fd-muted-foreground items-center empty:hidden","children":[false,[],["$","$L15",null,{"className":"ms-auto p-0","mode":"$undefined"}]]}],"$undefined"]}]]}]]}],false,["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}]]}]
8:["$","$1","c",{"children":[null,["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}]
9:["$","$1","c",{"children":["$L1e",null,["$","$L1f",null,{"children":["$L20",["$","$L21",null,{"promise":"$@22"}]]}]]}]
a:["$","$1","h",{"children":[null,[["$","$L23",null,{"children":"$L24"}],["$","meta",null,{"name":"next-size-adjust","content":""}]],["$","$L25",null,{"children":["$","div",null,{"hidden":true,"children":["$","$26",null,{"fallback":null,"children":"$L27"}]}]}]]}]
28:I[2853,["285","static/chunks/285-5cab2c46bf0cbdc3.js","697","static/chunks/697-8c55336afc55c2a1.js","324","static/chunks/324-9e7d7f5bb89b8982.js","870","static/chunks/app/docs/%5B%5B...slug%5D%5D/page-6a2f4c9a93e41015.js"],"TOCProvider"]
29:I[3448,["285","static/chunks/285-5cab2c46bf0cbdc3.js","697","static/chunks/697-8c55336afc55c2a1.js","324","static/chunks/324-9e7d7f5bb89b8982.js","870","static/chunks/app/docs/%5B%5B...slug%5D%5D/page-6a2f4c9a93e41015.js"],"PageTOCPopover"]
2a:I[3448,["285","static/chunks/285-5cab2c46bf0cbdc3.js","697","static/chunks/697-8c55336afc55c2a1.js","324","static/chunks/324-9e7d7f5bb89b8982.js","870","static/chunks/app/docs/%5B%5B...slug%5D%5D/page-6a2f4c9a93e41015.js"],"PageTOCPopoverTrigger"]
2b:I[3448,["285","static/chunks/285-5cab2c46bf0cbdc3.js","697","static/chunks/697-8c55336afc55c2a1.js","324","static/chunks/324-9e7d7f5bb89b8982.js","870","static/chunks/app/docs/%5B%5B...slug%5D%5D/page-6a2f4c9a93e41015.js"],"PageTOCPopoverContent"]
2c:I[2853,["285","static/chunks/285-5cab2c46bf0cbdc3.js","697","static/chunks/697-8c55336afc55c2a1.js","324","static/chunks/324-9e7d7f5bb89b8982.js","870","static/chunks/app/docs/%5B%5B...slug%5D%5D/page-6a2f4c9a93e41015.js"],"TOCScrollArea"]
2d:I[2853,["285","static/chunks/285-5cab2c46bf0cbdc3.js","697","static/chunks/697-8c55336afc55c2a1.js","324","static/chunks/324-9e7d7f5bb89b8982.js","870","static/chunks/app/docs/%5B%5B...slug%5D%5D/page-6a2f4c9a93e41015.js"],"TOCItems"]
2e:I[3448,["285","static/chunks/285-5cab2c46bf0cbdc3.js","697","static/chunks/697-8c55336afc55c2a1.js","324","static/chunks/324-9e7d7f5bb89b8982.js","870","static/chunks/app/docs/%5B%5B...slug%5D%5D/page-6a2f4c9a93e41015.js"],"PageBreadcrumb"]
1e:["$","$L28",null,{"toc":[{"depth":2,"url":"#教材与综合资源","title":"教材与综合资源"},{"depth":2,"url":"#第1课后训练概述与监督微调基础","title":"第1课：后训练概述与监督微调基础"},{"depth":3,"url":"#核心论文","title":"核心论文"},{"depth":3,"url":"#扩展阅读","title":"扩展阅读"},{"depth":2,"url":"#第2课sft-进阶与数据工程","title":"第2课：SFT 进阶与数据工程"},{"depth":3,"url":"#核心论文-1","title":"核心论文"},{"depth":3,"url":"#扩展阅读-1","title":"扩展阅读"},{"depth":2,"url":"#第3课偏好对齐dpo-及其变体","title":"第3课：偏好对齐——DPO 及其变体"},{"depth":3,"url":"#核心论文-2","title":"核心论文"},{"depth":3,"url":"#扩展阅读-2","title":"扩展阅读"},{"depth":2,"url":"#第4课rlhf-原理与推理强化学习grpo","title":"第4课：RLHF 原理与推理强化学习（GRPO）"},{"depth":3,"url":"#核心论文-3","title":"核心论文"},{"depth":3,"url":"#扩展阅读-3","title":"扩展阅读"},{"depth":2,"url":"#第5课模型压缩部署优化与能力扩展","title":"第5课：模型压缩、部署优化与能力扩展"},{"depth":3,"url":"#核心论文-4","title":"核心论文"},{"depth":3,"url":"#扩展阅读-4","title":"扩展阅读"},{"depth":2,"url":"#开源框架与工具","title":"开源框架与工具"},{"depth":2,"url":"#数据集索引","title":"数据集索引"},{"depth":2,"url":"#推荐学习路径","title":"推荐学习路径"}],"single":"$undefined","children":["$","div",null,{"id":"nd-page","className":"flex flex-1 w-full mx-auto max-w-(--fd-page-width) pt-(--fd-tocnav-height) pe-(--fd-toc-width)","children":[["$","$L29",null,{"children":[["$","$L2a",null,{}],["$","$L2b",null,{"children":["$undefined",["$","$L2c",null,{"children":["$","$L2d",null,{}]}],"$undefined"]}]]}],["$","article",null,{"children":[["$","$L2e",null,{}],[["$","h1",null,{"ref":"$undefined","children":"完整参考文献","className":"text-[1.75em] font-semibold"}],["$","p",null,{"ref":"$undefined","children":"按课次整理的全部推荐论文、教材和在线资源","className":"mb-8 text-lg text-fd-muted-foreground"}],["$","div",null,{"ref":"$undefined","children":[["$","h2",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"教材与综合资源","children":[["$","a",null,{"data-card":"","href":"#教材与综合资源","className":"peer","children":"教材与综合资源"}],["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-label":"Link to section","children":[[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}]],"$undefined"]}]]}],"\n",["$","div",null,{"className":"relative overflow-auto prose-no-margin my-6","children":["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{"children":"资源"}],["$","th",null,{"children":"作者"}],["$","th",null,{"children":"年份"}],["$","th",null,{"children":"说明"}]]}]}],["$","tbody",null,{"children":[["$","tr",null,{"children":[["$","td",null,{"children":["$","$Le",null,{"href":"https://arxiv.org/abs/2505.09388","children":"Qwen3 Technical Report"}]}],["$","td",null,{"children":"Qwen Team"}],["$","td",null,{"children":"2025"}],["$","td",null,{"children":"Qwen3 系列完整技术报告，贯穿全课程"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","$Le",null,{"href":"https://rlhfbook.com","children":"Reinforcement Learning from Human Feedback"}]}],["$","td",null,{"children":"Nathan Lambert"}],["$","td",null,{"children":"2025"}],["$","td",null,{"children":"第一本 RLHF 综合教材，免费获取"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","$Le",null,{"href":"https://github.com/huggingface/smol-course","children":"Hugging Face smol-course"}]}],["$","td",null,{"children":"Hugging Face"}],["$","td",null,{"children":"2024"}],["$","td",null,{"children":"开源实践迷你课程，涵盖 SFT、DPO、VLM"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","$Le",null,{"href":"https://github.com/huggingface/alignment-handbook","children":"Hugging Face Alignment Handbook"}]}],["$","td",null,{"children":"Hugging Face"}],["$","td",null,{"children":"2024"}],["$","td",null,{"children":"生产级 SFT → DPO/ORPO 流程方案"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","$Le",null,{"href":"https://cs336.stanford.edu/","children":"Stanford CS336: Language Modeling from Scratch"}]}],["$","td",null,{"children":"Stanford"}],["$","td",null,{"children":"2025"}],["$","td",null,{"children":"作业5涵盖对齐与推理 RL"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","$Le",null,{"href":"https://www.deeplearning.ai/","children":"Intro to Post-Training"}]}],["$","td",null,{"children":"DeepLearning.AI"}],["$","td",null,{"children":"2025"}],["$","td",null,{"children":"5模块视频课程"}]]}]]}]]}]}],"\n",["$","hr",null,{}],"\n",["$","h2",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"第1课后训练概述与监督微调基础","children":[["$","a",null,{"data-card":"","href":"#第1课后训练概述与监督微调基础","className":"peer","children":"第1课：后训练概述与监督微调基础"}],["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-label":"Link to section","children":[["$L2f","$L30"],"$undefined"]}]]}],"\n","$L31","\n","$L32","\n","$L33","\n","$L34","\n","$L35","\n","$L36","\n","$L37","\n","$L38","\n","$L39","\n","$L3a","\n","$L3b","\n","$L3c","\n","$L3d","\n","$L3e","\n","$L3f","\n","$L40","\n","$L41","\n","$L42","\n","$L43","\n","$L44","\n","$L45","\n","$L46","\n","$L47","\n","$L48","\n","$L49","\n","$L4a","\n","$L4b","\n","$L4c","\n","$L4d","\n","$L4e","\n","$L4f","\n","$L50","\n","$L51","\n","$L52","\n","$L53","\n","$L54","\n","$L55","\n","$L56","\n","$L57","\n","$L58","\n","$L59","\n","$L5a","\n","$L5b","\n","$L5c","\n","$L5d"],"className":"prose flex-1"}]],"$L5e","$L5f"],"className":"flex min-w-0 w-full flex-col gap-4 pt-8 px-4 md:px-6 md:mx-auto"}],"$L60"]}]}]
61:I[3448,["285","static/chunks/285-5cab2c46bf0cbdc3.js","697","static/chunks/697-8c55336afc55c2a1.js","324","static/chunks/324-9e7d7f5bb89b8982.js","870","static/chunks/app/docs/%5B%5B...slug%5D%5D/page-6a2f4c9a93e41015.js"],"PageFooter"]
62:I[3448,["285","static/chunks/285-5cab2c46bf0cbdc3.js","697","static/chunks/697-8c55336afc55c2a1.js","324","static/chunks/324-9e7d7f5bb89b8982.js","870","static/chunks/app/docs/%5B%5B...slug%5D%5D/page-6a2f4c9a93e41015.js"],"PageTOC"]
63:I[3734,["285","static/chunks/285-5cab2c46bf0cbdc3.js","697","static/chunks/697-8c55336afc55c2a1.js","324","static/chunks/324-9e7d7f5bb89b8982.js","870","static/chunks/app/docs/%5B%5B...slug%5D%5D/page-6a2f4c9a93e41015.js"],"I18nLabel"]
2f:["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}]
30:["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}]
31:["$","h3",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"核心论文","children":[["$","a",null,{"data-card":"","href":"#核心论文","className":"peer","children":"核心论文"}],["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-label":"Link to section","children":[[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}]],"$undefined"]}]]}]
32:["$","div",null,{"children":[["$","$Le",null,{"href":"https://arxiv.org/abs/2411.15124","children":[null,["$","h3",null,{"className":"not-prose mb-1 text-sm font-medium","children":"1. Tülu 3: Pushing Frontiers in Open Language Model Post-Training"}],null,["$","div",null,{"className":"text-sm text-fd-muted-foreground prose-no-margin empty:hidden","children":["$","p",null,{"children":"Lambert et al. (2024.11) — 最完整的开源后训练方案，SFT → DPO → RLVR 的黄金标准流程"}]}]],"data-card":true,"className":"block rounded-xl border bg-fd-card p-4 text-fd-card-foreground transition-colors @max-lg:col-span-full hover:bg-fd-accent/80"}],["$","$Le",null,{"href":"https://arxiv.org/abs/2106.09685","children":[null,["$","h3",null,{"className":"not-prose mb-1 text-sm font-medium","children":"2. LoRA: Low-Rank Adaptation of Large Language Models"}],null,["$","div",null,{"className":"text-sm text-fd-muted-foreground prose-no-margin empty:hidden","children":["$","p",null,{"children":"Hu et al. (2021) — 参数高效微调奠基论文，提出低秩分解训练方法"}]}]],"data-card":true,"className":"block rounded-xl border bg-fd-card p-4 text-fd-card-foreground transition-colors @max-lg:col-span-full hover:bg-fd-accent/80"}],["$","$Le",null,{"href":"https://arxiv.org/abs/2305.14314","children":[null,["$","h3",null,{"className":"not-prose mb-1 text-sm font-medium","children":"3. QLoRA: Efficient Finetuning of Quantized LLMs"}],null,["$","div",null,{"className":"text-sm text-fd-muted-foreground prose-no-margin empty:hidden","children":["$","p",null,{"children":"Dettmers et al. (2023) — NF4 量化 + LoRA，使大模型微调走向消费级 GPU"}]}]],"data-card":true,"className":"block rounded-xl border bg-fd-card p-4 text-fd-card-foreground transition-colors @max-lg:col-span-full hover:bg-fd-accent/80"}],["$","$Le",null,{"href":"https://arxiv.org/abs/2305.11206","children":[null,["$","h3",null,{"className":"not-prose mb-1 text-sm font-medium","children":"4. LIMA: Less Is More for Alignment"}],null,["$","div",null,{"className":"text-sm text-fd-muted-foreground prose-no-margin empty:hidden","children":["$","p",null,{"children":"Zhou et al. (2023) — 证明 1000 条高质量数据可超越 50000 条噪声数据"}]}]],"data-card":true,"className":"block rounded-xl border bg-fd-card p-4 text-fd-card-foreground transition-colors @max-lg:col-span-full hover:bg-fd-accent/80"}],["$","$Le",null,{"href":"https://arxiv.org/abs/2406.08464","children":[null,["$","h3",null,{"className":"not-prose mb-1 text-sm font-medium","children":"5. MAGPIE: Alignment Data Synthesis from Scratch"}],null,["$","div",null,{"className":"text-sm text-fd-muted-foreground prose-no-margin empty:hidden","children":["$","p",null,{"children":"Xu et al. (ICLR 2025) — 利用对齐模型的自动补全行为合成指令数据"}]}]],"data-card":true,"className":"block rounded-xl border bg-fd-card p-4 text-fd-card-foreground transition-colors @max-lg:col-span-full hover:bg-fd-accent/80"}]],"className":"grid grid-cols-2 gap-3 @container"}]
33:["$","h3",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"扩展阅读","children":[["$","a",null,{"data-card":"","href":"#扩展阅读","className":"peer","children":"扩展阅读"}],["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-label":"Link to section","children":[[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}]],"$undefined"]}]]}]
34:["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","$Le",null,{"href":"https://arxiv.org/abs/2305.16264","children":"Scaling Data-Constrained Language Models"}]," — Muennighoff et al. (2024), 数据规模与质量的系统研究"]}],"\n",["$","li",null,{"children":[["$","$Le",null,{"href":"https://arxiv.org/abs/2402.09353","children":"DoRA: Weight-Decomposed Low-Rank Adaptation"}]," — Liu et al. (2024), LoRA 的改进版本"]}],"\n",["$","li",null,{"children":[["$","$Le",null,{"href":"https://arxiv.org/abs/2406.06623","children":"Spectrum: Targeted Training on Signal to Noise Ratio"}]," — Verma et al. (2024), 基于信噪比的层选择"]}],"\n"]}]
35:["$","hr",null,{}]
36:["$","h2",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"第2课sft-进阶与数据工程","children":[["$","a",null,{"data-card":"","href":"#第2课sft-进阶与数据工程","className":"peer","children":"第2课：SFT 进阶与数据工程"}],["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-label":"Link to section","children":[[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}]],"$undefined"]}]]}]
37:["$","h3",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"核心论文-1","children":[["$","a",null,{"data-card":"","href":"#核心论文-1","className":"peer","children":"核心论文"}],["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-label":"Link to section","children":[[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}]],"$undefined"]}]]}]
38:["$","div",null,{"children":[["$","$Le",null,{"href":"https://arxiv.org/abs/2412.13337","children":[null,["$","h3",null,{"className":"not-prose mb-1 text-sm font-medium","children":"1. Unveiling the Secret Recipe: A Guide For Supervised Fine-Tuning Small LLMs"}],null,["$","div",null,{"className":"text-sm text-fd-muted-foreground prose-no-margin empty:hidden","children":["$","p",null,{"children":"Pareja et al. (2024.12) — 3B-7B 模型 SFT 的全面超参数指南"}]}]],"data-card":true,"className":"block rounded-xl border bg-fd-card p-4 text-fd-card-foreground transition-colors @max-lg:col-span-full hover:bg-fd-accent/80"}],["$","$Le",null,{"href":"https://arxiv.org/abs/2212.10560","children":[null,["$","h3",null,{"className":"not-prose mb-1 text-sm font-medium","children":"2. Self-Instruct: Aligning Language Models with Self-Generated Instructions"}],null,["$","div",null,{"className":"text-sm text-fd-muted-foreground prose-no-margin empty:hidden","children":["$","p",null,{"children":"Wang et al. (2023) — 指令数据合成的开创性工作"}]}]],"data-card":true,"className":"block rounded-xl border bg-fd-card p-4 text-fd-card-foreground transition-colors @max-lg:col-span-full hover:bg-fd-accent/80"}],["$","$Le",null,{"href":"https://arxiv.org/abs/2306.05685","children":[null,["$","h3",null,{"className":"not-prose mb-1 text-sm font-medium","children":"3. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena"}],null,["$","div",null,{"className":"text-sm text-fd-muted-foreground prose-no-margin empty:hidden","children":["$","p",null,{"children":"Zheng et al. (NeurIPS 2023) — LLM-as-Judge 评估框架"}]}]],"data-card":true,"className":"block rounded-xl border bg-fd-card p-4 text-fd-card-foreground transition-colors @max-lg:col-span-full hover:bg-fd-accent/80"}],["$","$Le",null,{"href":"https://arxiv.org/abs/2305.14233","children":[null,["$","h3",null,{"className":"not-prose mb-1 text-sm font-medium","children":"4. UltraChat: A Large-scale Auto-generated Multi-turn Instruction Dataset"}],null,["$","div",null,{"className":"text-sm text-fd-muted-foreground prose-no-margin empty:hidden","children":["$","p",null,{"children":"Ding et al. (2023) — 高质量多轮对话数据集构建方法"}]}]],"data-card":true,"className":"block rounded-xl border bg-fd-card p-4 text-fd-card-foreground transition-colors @max-lg:col-span-full hover:bg-fd-accent/80"}],["$","$Le",null,{"href":"https://arxiv.org/abs/2312.15685","children":[null,["$","h3",null,{"className":"not-prose mb-1 text-sm font-medium","children":"5. Deita: Data-Efficient Instruction Tuning Alignment"}],null,["$","div",null,{"className":"text-sm text-fd-muted-foreground prose-no-margin empty:hidden","children":["$","p",null,{"children":"Liu et al. (2024) — 数据高效选择策略"}]}]],"data-card":true,"className":"block rounded-xl border bg-fd-card p-4 text-fd-card-foreground transition-colors @max-lg:col-span-full hover:bg-fd-accent/80"}]],"className":"grid grid-cols-2 gap-3 @container"}]
39:["$","h3",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"扩展阅读-1","children":[["$","a",null,{"data-card":"","href":"#扩展阅读-1","className":"peer","children":"扩展阅读"}],["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-label":"Link to section","children":[[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}]],"$undefined"]}]]}]
3a:["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","$Le",null,{"href":"https://crfm.stanford.edu/2023/03/13/alpaca.html","children":"Alpaca: A Strong, Replicable Instruction-Following Model"}]," — Stanford (2023), 指令微调的早期里程碑"]}],"\n",["$","li",null,{"children":[["$","$Le",null,{"href":"https://arxiv.org/abs/2410.01220","children":"GRAPE: Generalizing Robot Policy via Preference Alignment"}]," — 2025, 适配基座模型分布的数据选择"]}],"\n",["$","li",null,{"children":[["$","$Le",null,{"href":"https://arxiv.org/abs/2304.12244","children":"WizardLM: Empowering Large Language Models to Follow Complex Instructions"}]," — Xu et al. (2023)"]}],"\n"]}]
3b:["$","hr",null,{}]
3c:["$","h2",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"第3课偏好对齐dpo-及其变体","children":[["$","a",null,{"data-card":"","href":"#第3课偏好对齐dpo-及其变体","className":"peer","children":"第3课：偏好对齐——DPO 及其变体"}],["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-label":"Link to section","children":[[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}]],"$undefined"]}]]}]
3d:["$","h3",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"核心论文-2","children":[["$","a",null,{"data-card":"","href":"#核心论文-2","className":"peer","children":"核心论文"}],["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-label":"Link to section","children":[[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}]],"$undefined"]}]]}]
3e:["$","div",null,{"children":[["$","$Le",null,{"href":"https://arxiv.org/abs/2305.18290","children":[null,["$","h3",null,{"className":"not-prose mb-1 text-sm font-medium","children":"1. Direct Preference Optimization: Your Language Model is Secretly a Reward Model"}],null,["$","div",null,{"className":"text-sm text-fd-muted-foreground prose-no-margin empty:hidden","children":["$","p",null,{"children":"Rafailov et al. (NeurIPS 2023) — DPO 奠基论文，必读"}]}]],"data-card":true,"className":"block rounded-xl border bg-fd-card p-4 text-fd-card-foreground transition-colors @max-lg:col-span-full hover:bg-fd-accent/80"}],["$","$Le",null,{"href":"https://arxiv.org/abs/2405.14734","children":[null,["$","h3",null,{"className":"not-prose mb-1 text-sm font-medium","children":"2. SimPO: Simple Preference Optimization with a Reference-Free Reward"}],null,["$","div",null,{"className":"text-sm text-fd-muted-foreground prose-no-margin empty:hidden","children":["$","p",null,{"children":"Meng et al. (NeurIPS 2024) — 无参考模型对齐，<10B 模型 AlpacaEval 2 排名第一"}]}]],"data-card":true,"className":"block rounded-xl border bg-fd-card p-4 text-fd-card-foreground transition-colors @max-lg:col-span-full hover:bg-fd-accent/80"}],["$","$Le",null,{"href":"https://arxiv.org/abs/2402.01306","children":[null,["$","h3",null,{"className":"not-prose mb-1 text-sm font-medium","children":"3. KTO: Model Alignment as Prospect Theoretic Optimization"}],null,["$","div",null,{"className":"text-sm text-fd-muted-foreground prose-no-margin empty:hidden","children":["$","p",null,{"children":"Ethayarajh et al. (2024) — 行为经济学与 LLM 对齐的关联"}]}]],"data-card":true,"className":"block rounded-xl border bg-fd-card p-4 text-fd-card-foreground transition-colors @max-lg:col-span-full hover:bg-fd-accent/80"}],["$","$Le",null,{"href":"https://arxiv.org/abs/2406.09279","children":[null,["$","h3",null,{"className":"not-prose mb-1 text-sm font-medium","children":"4. Unpacking DPO and PPO: Disentangling Best Practices"}],null,["$","div",null,{"className":"text-sm text-fd-muted-foreground prose-no-margin empty:hidden","children":["$","p",null,{"children":"Ivison et al. (2024.06) — DPO 与 PPO 的系统性控制实验"}]}]],"data-card":true,"className":"block rounded-xl border bg-fd-card p-4 text-fd-card-foreground transition-colors @max-lg:col-span-full hover:bg-fd-accent/80"}],["$","$Le",null,{"href":"https://arxiv.org/abs/2410.15595","children":[null,["$","h3",null,{"className":"not-prose mb-1 text-sm font-medium","children":"5. A Comprehensive Survey of Direct Preference Optimization"}],null,["$","div",null,{"className":"text-sm text-fd-muted-foreground prose-no-margin empty:hidden","children":["$","p",null,{"children":"(2024.10, 更新至2025) — 覆盖 20+ DPO 变体的综述"}]}]],"data-card":true,"className":"block rounded-xl border bg-fd-card p-4 text-fd-card-foreground transition-colors @max-lg:col-span-full hover:bg-fd-accent/80"}]],"className":"grid grid-cols-2 gap-3 @container"}]
3f:["$","h3",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"扩展阅读-2","children":[["$","a",null,{"data-card":"","href":"#扩展阅读-2","className":"peer","children":"扩展阅读"}],["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-label":"Link to section","children":[[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}]],"$undefined"]}]]}]
40:["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","$Le",null,{"href":"https://arxiv.org/abs/2403.07691","children":"ORPO: Monolithic Preference Optimization without Reference Model"}]," — Hong et al. (2024), 将 SFT 和对齐合并为单一损失"]}],"\n",["$","li",null,{"children":[["$","$Le",null,{"href":"https://arxiv.org/abs/2310.12036","children":"IPO: A General Theoretical Paradigm to Understand Learning from Human Preferences"}]," — Azar et al. (Google DeepMind, 2023)"]}],"\n",["$","li",null,{"children":[["$","$Le",null,{"href":"https://arxiv.org/abs/2212.08073","children":"Constitutional AI: Harmlessness from AI Feedback"}]," — Bai et al. (Anthropic, 2022)"]}],"\n"]}]
41:["$","hr",null,{}]
42:["$","h2",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"第4课rlhf-原理与推理强化学习grpo","children":[["$","a",null,{"data-card":"","href":"#第4课rlhf-原理与推理强化学习grpo","className":"peer","children":"第4课：RLHF 原理与推理强化学习（GRPO）"}],["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-label":"Link to section","children":[[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}]],"$undefined"]}]]}]
43:["$","h3",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"核心论文-3","children":[["$","a",null,{"data-card":"","href":"#核心论文-3","className":"peer","children":"核心论文"}],["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-label":"Link to section","children":[[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}]],"$undefined"]}]]}]
44:["$","div",null,{"children":[["$","$Le",null,{"href":"https://arxiv.org/abs/2203.02155","children":[null,["$","h3",null,{"className":"not-prose mb-1 text-sm font-medium","children":"1. Training Language Models to Follow Instructions with Human Feedback"}],null,["$","div",null,{"className":"text-sm text-fd-muted-foreground prose-no-margin empty:hidden","children":["$","p",null,{"children":"Ouyang et al. (OpenAI, 2022) — InstructGPT，确立 RLHF 三阶段流程"}]}]],"data-card":true,"className":"block rounded-xl border bg-fd-card p-4 text-fd-card-foreground transition-colors @max-lg:col-span-full hover:bg-fd-accent/80"}],["$","$Le",null,{"href":"https://arxiv.org/abs/2501.12948","children":[null,["$","h3",null,{"className":"not-prose mb-1 text-sm font-medium","children":"2. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via RL"}],null,["$","div",null,{"className":"text-sm text-fd-muted-foreground prose-no-margin empty:hidden","children":["$","p",null,{"children":"DeepSeek-AI (2025.01) — 推理涌现的里程碑论文"}]}]],"data-card":true,"className":"block rounded-xl border bg-fd-card p-4 text-fd-card-foreground transition-colors @max-lg:col-span-full hover:bg-fd-accent/80"}],["$","$Le",null,{"href":"https://arxiv.org/abs/2402.03300","children":[null,["$","h3",null,{"className":"not-prose mb-1 text-sm font-medium","children":"3. DeepSeekMath: Pushing the Limits of Mathematical Reasoning"}],null,["$","div",null,{"className":"text-sm text-fd-muted-foreground prose-no-margin empty:hidden","children":["$","p",null,{"children":"Shao et al. (2024.02) — GRPO 算法的原始论文"}]}]],"data-card":true,"className":"block rounded-xl border bg-fd-card p-4 text-fd-card-foreground transition-colors @max-lg:col-span-full hover:bg-fd-accent/80"}],["$","$Le",null,{"href":"https://arxiv.org/abs/2408.03314","children":[null,["$","h3",null,{"className":"not-prose mb-1 text-sm font-medium","children":"4. Scaling LLM Test-Time Compute Optimally"}],null,["$","div",null,{"className":"text-sm text-fd-muted-foreground prose-no-margin empty:hidden","children":["$","p",null,{"children":"Snell et al. (2024.08) — 测试时计算扩展的理论基础"}]}]],"data-card":true,"className":"block rounded-xl border bg-fd-card p-4 text-fd-card-foreground transition-colors @max-lg:col-span-full hover:bg-fd-accent/80"}],["$","$Le",null,{"href":"https://arxiv.org/abs/2503.14476","children":[null,["$","h3",null,{"className":"not-prose mb-1 text-sm font-medium","children":"5. DAPO: An Open-Source LLM Reinforcement Learning System at Scale"}],null,["$","div",null,{"className":"text-sm text-fd-muted-foreground prose-no-margin empty:hidden","children":["$","p",null,{"children":"Yu et al. (ByteDance, 2025.03) — GRPO 的实用改进，完全开源"}]}]],"data-card":true,"className":"block rounded-xl border bg-fd-card p-4 text-fd-card-foreground transition-colors @max-lg:col-span-full hover:bg-fd-accent/80"}]],"className":"grid grid-cols-2 gap-3 @container"}]
45:["$","h3",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"扩展阅读-3","children":[["$","a",null,{"data-card":"","href":"#扩展阅读-3","className":"peer","children":"扩展阅读"}],["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-label":"Link to section","children":[[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}]],"$undefined"]}]]}]
46:["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","$Le",null,{"href":"https://arxiv.org/abs/2310.12773","children":"Safe RLHF: Safe Reinforcement Learning from Human Feedback"}]," — 北京大学对齐团队 (ICLR 2024), 解耦有用性和无害性"]}],"\n",["$","li",null,{"children":[["$","$Le",null,{"href":"https://arxiv.org/abs/2505.09388","children":"Qwen3 Technical Report"}]," — Qwen Team (2025), Section 4.2 详述四阶段后训练"]}],"\n",["$","li",null,{"children":[["$","$Le",null,{"href":"https://arxiv.org/abs/2503.20783","children":"Dr. GRPO"}]," — MIT (2025), 去除长度偏差的 GRPO 改进"]}],"\n",["$","li",null,{"children":[["$","$Le",null,{"href":"https://arxiv.org/abs/2501.03262","children":"REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models"}]," — Hu (2025)"]}],"\n",["$","li",null,{"children":[["$","$Le",null,{"href":"https://arxiv.org/abs/1707.06347","children":"Proximal Policy Optimization Algorithms"}]," — Schulman et al. (OpenAI, 2017), PPO 原始论文"]}],"\n"]}]
47:["$","hr",null,{}]
48:["$","h2",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"第5课模型压缩部署优化与能力扩展","children":[["$","a",null,{"data-card":"","href":"#第5课模型压缩部署优化与能力扩展","className":"peer","children":"第5课：模型压缩、部署优化与能力扩展"}],["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-label":"Link to section","children":[[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}]],"$undefined"]}]]}]
49:["$","h3",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"核心论文-4","children":[["$","a",null,{"data-card":"","href":"#核心论文-4","className":"peer","children":"核心论文"}],["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-label":"Link to section","children":[[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}]],"$undefined"]}]]}]
4a:["$","div",null,{"children":[["$","$Le",null,{"href":"https://arxiv.org/abs/2210.17323","children":[null,["$","h3",null,{"className":"not-prose mb-1 text-sm font-medium","children":"1. GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"}],null,["$","div",null,{"className":"text-sm text-fd-muted-foreground prose-no-margin empty:hidden","children":["$","p",null,{"children":"Frantar et al. (2023) — 基于 Hessian 信息的大模型量化方法"}]}]],"data-card":true,"className":"block rounded-xl border bg-fd-card p-4 text-fd-card-foreground transition-colors @max-lg:col-span-full hover:bg-fd-accent/80"}],["$","$Le",null,{"href":"https://arxiv.org/abs/2306.00978","children":[null,["$","h3",null,{"className":"not-prose mb-1 text-sm font-medium","children":"2. AWQ: Activation-aware Weight Quantization"}],null,["$","div",null,{"className":"text-sm text-fd-muted-foreground prose-no-margin empty:hidden","children":["$","p",null,{"children":"Lin et al. (MLSys 2024) — 激活感知量化，保护重要通道"}]}]],"data-card":true,"className":"block rounded-xl border bg-fd-card p-4 text-fd-card-foreground transition-colors @max-lg:col-span-full hover:bg-fd-accent/80"}],["$","$Le",null,{"href":"https://arxiv.org/abs/2304.08485","children":[null,["$","h3",null,{"className":"not-prose mb-1 text-sm font-medium","children":"3. Visual Instruction Tuning (LLaVA)"}],null,["$","div",null,{"className":"text-sm text-fd-muted-foreground prose-no-margin empty:hidden","children":["$","p",null,{"children":"Liu et al. (NeurIPS 2023) — VLM 两阶段训练范式"}]}]],"data-card":true,"className":"block rounded-xl border bg-fd-card p-4 text-fd-card-foreground transition-colors @max-lg:col-span-full hover:bg-fd-accent/80"}],["$","$Le",null,{"href":"https://arxiv.org/abs/2409.00920","children":[null,["$","h3",null,{"className":"not-prose mb-1 text-sm font-medium","children":"4. ToolACE: Winning the Points of LLM Function Calling"}],null,["$","div",null,{"className":"text-sm text-fd-muted-foreground prose-no-margin empty:hidden","children":["$","p",null,{"children":"Liu et al. (ICLR 2025) — 8B 模型函数调用超越 GPT-4"}]}]],"data-card":true,"className":"block rounded-xl border bg-fd-card p-4 text-fd-card-foreground transition-colors @max-lg:col-span-full hover:bg-fd-accent/80"}],["$","$Le",null,{"href":"https://arxiv.org/abs/2501.12948","children":[null,["$","h3",null,{"className":"not-prose mb-1 text-sm font-medium","children":"5. DeepSeek-R1 (蒸馏部分)"}],null,["$","div",null,{"className":"text-sm text-fd-muted-foreground prose-no-margin empty:hidden","children":["$","p",null,{"children":"DeepSeek-AI (2025.01) — 将推理能力蒸馏到小模型"}]}]],"data-card":true,"className":"block rounded-xl border bg-fd-card p-4 text-fd-card-foreground transition-colors @max-lg:col-span-full hover:bg-fd-accent/80"}]],"className":"grid grid-cols-2 gap-3 @container"}]
4b:["$","h3",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"扩展阅读-4","children":[["$","a",null,{"data-card":"","href":"#扩展阅读-4","className":"peer","children":"扩展阅读"}],["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-label":"Link to section","children":[[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}]],"$undefined"]}]]}]
4c:["$","p",null,{"children":[["$","strong",null,{"children":"量化方向"}],"："]}]
4d:["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","$Le",null,{"href":"https://arxiv.org/abs/2208.07339","children":"LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}]," — Dettmers et al. (2022)"]}],"\n",["$","li",null,{"children":[["$","$Le",null,{"href":"https://arxiv.org/abs/2306.07629","children":"SqueezeLLM: Dense-and-Sparse Quantization"}]," — Kim et al. (2023)"]}],"\n"]}]
4e:["$","p",null,{"children":[["$","strong",null,{"children":"多模态方向"}],"："]}]
4f:["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","$Le",null,{"href":"https://arxiv.org/abs/2408.03326","children":"LLaVA-OneVision: Easy Visual Task Transfer"}]," — Li et al. (2024)"]}],"\n",["$","li",null,{"children":[["$","$Le",null,{"href":"https://arxiv.org/abs/2312.14238","children":"InternVL: Scaling Up Vision Foundation Models"}]," — Chen et al. (2023)"]}],"\n",["$","li",null,{"children":[["$","$Le",null,{"href":"https://arxiv.org/abs/2312.00849","children":"RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment"}]," — Yu et al. (2023)"]}],"\n"]}]
50:["$","p",null,{"children":[["$","strong",null,{"children":"工具使用方向"}],"："]}]
51:["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","$Le",null,{"href":"https://arxiv.org/abs/2305.15334","children":"Gorilla: Large Language Model Connected with Massive APIs"}]," — Patil et al. (2023)"]}],"\n",["$","li",null,{"children":[["$","$Le",null,{"href":"https://arxiv.org/abs/2302.04761","children":"Toolformer: Language Models Can Teach Themselves to Use Tools"}]," — Schick et al. (2023)"]}],"\n",["$","li",null,{"children":[["$","$Le",null,{"href":"https://arxiv.org/abs/2210.03629","children":"ReAct: Synergizing Reasoning and Acting in Language Models"}]," — Yao et al. (2022)"]}],"\n"]}]
52:["$","p",null,{"children":[["$","strong",null,{"children":"知识蒸馏方向"}],"："]}]
53:["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","$Le",null,{"href":"https://arxiv.org/abs/1503.02531","children":"Distilling the Knowledge in a Neural Network"}]," — Hinton et al. (2015), 经典蒸馏论文"]}],"\n"]}]
54:["$","hr",null,{}]
55:["$","h2",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"开源框架与工具","children":[["$","a",null,{"data-card":"","href":"#开源框架与工具","className":"peer","children":"开源框架与工具"}],["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-label":"Link to section","children":[[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}]],"$undefined"]}]]}]
56:["$","div",null,{"className":"relative overflow-auto prose-no-margin my-6","children":["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{"children":"框架"}],["$","th",null,{"children":"用途"}],["$","th",null,{"children":"GitHub"}],["$","th",null,{"children":"课程中使用"}]]}]}],["$","tbody",null,{"children":[["$","tr",null,{"children":[["$","td",null,{"children":"Hugging Face Transformers"}],["$","td",null,{"children":"模型加载与推理"}],["$","td",null,{"children":["$","$Le",null,{"href":"https://github.com/huggingface/transformers","children":"github.com/huggingface/transformers"}]}],["$","td",null,{"children":"全部课次"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"TRL"}],["$","td",null,{"children":"SFT/DPO/GRPO 训练"}],["$","td",null,{"children":["$","$Le",null,{"href":"https://github.com/huggingface/trl","children":"github.com/huggingface/trl"}]}],["$","td",null,{"children":"第1-4课"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"PEFT"}],["$","td",null,{"children":"LoRA/QLoRA 适配器"}],["$","td",null,{"children":["$","$Le",null,{"href":"https://github.com/huggingface/peft","children":"github.com/huggingface/peft"}]}],["$","td",null,{"children":"第1-4课"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"bitsandbytes"}],["$","td",null,{"children":"量化工具"}],["$","td",null,{"children":["$","$Le",null,{"href":"https://github.com/bitsandbytes-foundation/bitsandbytes","children":"github.com/bitsandbytes-foundation/bitsandbytes"}]}],["$","td",null,{"children":"第1、5课"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"LLaMA-Factory"}],["$","td",null,{"children":"一站式微调框架"}],["$","td",null,{"children":["$","$Le",null,{"href":"https://github.com/hiyouga/LLaMA-Factory","children":"github.com/hiyouga/LLaMA-Factory"}]}],["$","td",null,{"children":"第5课选做"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"vLLM"}],["$","td",null,{"children":"高性能推理引擎"}],["$","td",null,{"children":["$","$Le",null,{"href":"https://github.com/vllm-project/vllm","children":"github.com/vllm-project/vllm"}]}],["$","td",null,{"children":"第4-5课"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"OpenRLHF"}],["$","td",null,{"children":"分布式 RLHF 框架"}],["$","td",null,{"children":["$","$Le",null,{"href":"https://github.com/OpenRLHF/OpenRLHF","children":"github.com/OpenRLHF/OpenRLHF"}]}],["$","td",null,{"children":"参考"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"veRL"}],["$","td",null,{"children":"大规模 GRPO 训练"}],["$","td",null,{"children":["$","$Le",null,{"href":"https://github.com/volcengine/verl","children":"github.com/volcengine/verl"}]}],["$","td",null,{"children":"参考"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"Outlines"}],["$","td",null,{"children":"约束解码"}],["$","td",null,{"children":["$","$Le",null,{"href":"https://github.com/outlines-dev/outlines","children":"github.com/outlines-dev/outlines"}]}],["$","td",null,{"children":"第5课"}]]}]]}]]}]}]
57:["$","hr",null,{}]
58:["$","h2",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"数据集索引","children":[["$","a",null,{"data-card":"","href":"#数据集索引","className":"peer","children":"数据集索引"}],["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-label":"Link to section","children":[[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}]],"$undefined"]}]]}]
59:["$","div",null,{"className":"relative overflow-auto prose-no-margin my-6","children":["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{"children":"数据集"}],["$","th",null,{"children":"类型"}],["$","th",null,{"children":"规模"}],["$","th",null,{"children":"课次"}],["$","th",null,{"children":"链接"}]]}]}],["$","tbody",null,{"children":[["$","tr",null,{"children":[["$","td",null,{"children":"UltraChat-200K"}],["$","td",null,{"children":"多轮对话"}],["$","td",null,{"children":"200K"}],["$","td",null,{"children":"第1课"}],["$","td",null,{"children":["$","$Le",null,{"href":"https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k","children":"HuggingFace"}]}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"UltraFeedback"}],["$","td",null,{"children":"偏好数据"}],["$","td",null,{"children":"64K"}],["$","td",null,{"children":"第3课"}],["$","td",null,{"children":["$","$Le",null,{"href":"https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized","children":"HuggingFace"}]}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"GSM8K"}],["$","td",null,{"children":"数学推理"}],["$","td",null,{"children":"8.8K"}],["$","td",null,{"children":"第4课"}],["$","td",null,{"children":["$","$Le",null,{"href":"https://huggingface.co/datasets/openai/gsm8k","children":"HuggingFace"}]}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"COIG-CQIA"}],["$","td",null,{"children":"中文指令"}],["$","td",null,{"children":"多种"}],["$","td",null,{"children":"第2课"}],["$","td",null,{"children":["$","$Le",null,{"href":"https://huggingface.co/datasets/m-a-p/COIG-CQIA","children":"HuggingFace"}]}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"PKU-SafeRLHF"}],["$","td",null,{"children":"安全偏好"}],["$","td",null,{"children":"361K"}],["$","td",null,{"children":"期末项目"}],["$","td",null,{"children":["$","$Le",null,{"href":"https://huggingface.co/datasets/PKU-Alignment/PKU-SafeRLHF","children":"HuggingFace"}]}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"LLaVA-Instruct-150K"}],["$","td",null,{"children":"视觉指令"}],["$","td",null,{"children":"150K"}],["$","td",null,{"children":"期末项目"}],["$","td",null,{"children":["$","$Le",null,{"href":"https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K","children":"HuggingFace"}]}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"Glaive FC v2"}],["$","td",null,{"children":"函数调用"}],["$","td",null,{"children":"113K"}],["$","td",null,{"children":"第5课/期末"}],["$","td",null,{"children":["$","$Le",null,{"href":"https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2","children":"HuggingFace"}]}]]}]]}]]}]}]
5a:["$","hr",null,{}]
5b:["$","h2",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"推荐学习路径","children":[["$","a",null,{"data-card":"","href":"#推荐学习路径","className":"peer","children":"推荐学习路径"}],["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-label":"Link to section","children":[[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}]],"$undefined"]}]]}]
5c:["$","div",null,{"ref":"$undefined","className":"flex gap-2 my-4 rounded-xl border bg-fd-card p-3 ps-1 text-sm text-fd-card-foreground shadow-md","style":{"--callout-color":"var(--color-fd-info, var(--color-fd-muted))"},"children":[["$","div",null,{"role":"none","className":"w-0.5 bg-(--callout-color)/50 rounded-sm"}],["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide size-5 -me-0.5 fill-(--callout-color) text-fd-card","children":[[["$","circle","1mglay",{"cx":"12","cy":"12","r":"10"}],["$","path","1dtifu",{"d":"M12 16v-4"}],["$","path","e9boi3",{"d":"M12 8h.01"}]],"$undefined"]}],["$","div",null,{"className":"flex flex-col gap-2 min-w-0 flex-1","children":["$undefined",["$","div",null,{"className":"text-fd-muted-foreground prose-no-margin empty:hidden","children":["$","p",null,{"children":[["$","strong",null,{"children":"论文阅读建议"}],"：每课精读 1-2 篇核心论文（标记为\"必读\"的），其余泛读 Abstract 和实验部分。整个课程建议精读 8-10 篇论文，泛读 15-20 篇。"]}]}]]}]]}]
5d:["$","div",null,{"className":"relative overflow-auto prose-no-margin my-6","children":["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{"children":"阶段"}],["$","th",null,{"children":"重点论文"}],["$","th",null,{"children":"阅读优先级"}]]}]}],["$","tbody",null,{"children":[["$","tr",null,{"children":[["$","td",null,{"children":"入门（第1-2课）"}],["$","td",null,{"children":"LoRA, QLoRA, LIMA"}],["$","td",null,{"children":"必读"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"对齐（第3课）"}],["$","td",null,{"children":"DPO, SimPO"}],["$","td",null,{"children":"必读"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"推理（第4课）"}],["$","td",null,{"children":"DeepSeek-R1, GRPO"}],["$","td",null,{"children":"必读"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"部署（第5课）"}],["$","td",null,{"children":"GPTQ 或 AWQ (选一), LLaVA"}],["$","td",null,{"children":"推荐"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"综合（第6课）"}],["$","td",null,{"children":"Tülu 3, Qwen3 Technical Report"}],["$","td",null,{"children":"必读"}]]}]]}]]}]}]
5e:["$","div",null,{"className":"flex flex-row flex-wrap items-center justify-between gap-4 empty:hidden","children":["$undefined","$undefined"]}]
5f:["$","$L61",null,{"items":"$undefined"}]
60:["$","$L62",null,{"children":["$undefined",["$","h3",null,{"className":"inline-flex items-center gap-1.5 text-sm text-fd-muted-foreground","children":[["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide size-4","children":[[["$","path","olowqp",{"d":"M15 18H3"}],["$","path","16j9eg",{"d":"M17 6H3"}],["$","path","2avoz0",{"d":"M21 12H3"}]],"$undefined"]}],["$","$L63",null,{"label":"toc"}]]}],["$","$L2c",null,{"children":["$","$L2d",null,{}]}],"$undefined"]}]
24:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
20:null
22:{"metadata":[["$","title","0",{"children":"完整参考文献 | LLM 后训练课程"}],["$","meta","1",{"name":"description","content":"按课次整理的全部推荐论文、教材和在线资源"}]],"error":null,"digest":"$undefined"}
27:"$22:metadata"
